{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>OUTLIER DETECTION AND HANDLING</b> https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Extraction & Feature Engineering\n",
    "\n",
    "Feature Transformation\n",
    "\n",
    "Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "    \n",
    "We call vectorization the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n",
    "\n",
    "We will use CountVectorizer to \"convert text into a matrix of token counts\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BAG OF WORDS:<br />\n",
    "https://machinelearningmastery.com/gentle-introduction-bag-words-model/\n",
    "\n",
    "TF-IDF:<br />\n",
    "https://www.commonlounge.com/discussion/99e86c9c15bb4d23a30b111b23e7b7b1\n",
    "\n",
    "CODE EXAMPLE:<br />\n",
    "https://machinelearningmastery.com/prepare-text-data-machine-learning-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T07:17:51.699983Z",
     "start_time": "2018-06-19T07:17:43.647058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nvocab.fit(lst_text)\\n\\ndtm = vocab.transform()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "lst_text=['it was the best of times','it was the worst of times',\\\n",
    "          'it was the age of wisdom','it was the age of foolishness']\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.\n",
    "vocab = CountVectorizer()\n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "dtm = vocab.fit_transform(lst_text)\n",
    "\n",
    "'''\n",
    "vocab.fit(lst_text)\n",
    "\n",
    "dtm = vocab.transform()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T07:17:56.568919Z",
     "start_time": "2018-06-19T07:17:56.561936Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'it': 3,\n",
       " 'was': 7,\n",
       " 'the': 5,\n",
       " 'best': 1,\n",
       " 'of': 4,\n",
       " 'times': 6,\n",
       " 'worst': 9,\n",
       " 'age': 0,\n",
       " 'wisdom': 8,\n",
       " 'foolishness': 2}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T07:19:26.013354Z",
     "start_time": "2018-06-19T07:19:26.007364Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(4, 10)\n",
      "[[0 1 0 1 1 1 1 1 0 0]\n",
      " [0 0 0 1 1 1 1 1 0 1]\n",
      " [1 0 0 1 1 1 0 1 1 0]\n",
      " [1 0 1 1 1 1 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(type(dtm))\n",
    "\n",
    "print(dtm.shape)\n",
    "\n",
    "# print(dtm)\n",
    "\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T02:21:48.644492Z",
     "start_time": "2018-06-19T02:21:48.623596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 5, 'was': 16, 'the': 11, 'best': 2, 'of': 7, 'times': 15, 'it was': 6, 'was the': 17, 'the best': 13, 'best of': 3, 'of times': 9, 'worst': 19, 'the worst': 14, 'worst of': 20, 'age': 0, 'wisdom': 18, 'the age': 12, 'age of': 1, 'of wisdom': 10, 'foolishness': 4, 'of foolishness': 8}\n",
      "[[0 0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 1 0 0 0]\n",
      " [0 0 0 0 0 1 1 1 0 1 0 1 0 0 1 1 1 1 0 1 1]\n",
      " [1 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 0]\n",
      " [1 1 0 0 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 2-grams\n",
    "\n",
    "vocab = CountVectorizer(ngram_range=[1,2])\n",
    "\n",
    "dtm = vocab.fit_transform(lst_text)\n",
    "\n",
    "print(vocab.vocabulary_)\n",
    "\n",
    "print(dtm.toarray()) # convert sparse matrix to nparray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Summary:</b>\n",
    "<ul>\n",
    "    <li> <code>vect.fit(lst_text)</code> <b>learns the vocabulary</b>\n",
    "    <li> <code>vect.transform(lst_text)</code> <b>uses the fitted vocabulary</b> to build a <b>document-term matrix</b>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T02:32:40.704703Z",
     "start_time": "2018-06-19T02:32:40.695308Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'it': 3, 'was': 7, 'the': 5, 'best': 1, 'of': 4, 'times': 6, 'worst': 9, 'age': 0, 'wisdom': 8, 'foolishness': 2}\n",
      "**************************************************\n",
      "[[0.         0.60735961 0.         0.31694544 0.31694544 0.31694544\n",
      "  0.4788493  0.31694544 0.         0.        ]\n",
      " [0.         0.         0.         0.31694544 0.31694544 0.31694544\n",
      "  0.4788493  0.31694544 0.         0.60735961]\n",
      " [0.4788493  0.         0.         0.31694544 0.31694544 0.31694544\n",
      "  0.         0.31694544 0.60735961 0.        ]\n",
      " [0.4788493  0.         0.60735961 0.31694544 0.31694544 0.31694544\n",
      "  0.         0.31694544 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# list of text documents\n",
    "lst_text=['It was the best of times','it was the worst of times',\\\n",
    "          'it was the age of wisdom','it was the age of foolishness']\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "dtm = vectorizer.fit_transform(lst_text)\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "print('*'*50)\n",
    "\n",
    "print(dtm.toarray()) # convert sparse matrix to nparray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization & Changing Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max Scaling (Column Normalization)\n",
    "\n",
    "Standard Scaling (Z score normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T08:06:57.768353Z",
     "start_time": "2018-06-19T08:06:57.761380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6]\n",
      "**************************************************\n",
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3],[4,5,6]])\n",
    "\n",
    "print(a.reshape(6))\n",
    "\n",
    "print('*'*50)\n",
    "\n",
    "print(a.reshape(3,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T08:05:42.391216Z",
     "start_time": "2018-06-19T08:05:42.381242Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.],\n",
       "       [  1.],\n",
       "       [  0.],\n",
       "       [ -1.],\n",
       "       [  2.],\n",
       "       [  1.],\n",
       "       [  3.],\n",
       "       [ -2.],\n",
       "       [  4.],\n",
       "       [100.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([1,1,0,-1,2,1,3,-2,4,100], dtype='f').reshape(-1,1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T03:00:34.649588Z",
     "start_time": "2018-06-19T03:00:34.639912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02941177],\n",
       "       [0.02941177],\n",
       "       [0.01960784],\n",
       "       [0.00980392],\n",
       "       [0.03921569],\n",
       "       [0.02941177],\n",
       "       [0.04901961],\n",
       "       [0.        ],\n",
       "       [0.05882353],\n",
       "       [1.0000001 ]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Min-Max Scaling\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "MinMaxScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T03:00:36.436909Z",
     "start_time": "2018-06-19T03:00:36.417676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02941176],\n",
       "       [0.02941176],\n",
       "       [0.01960784],\n",
       "       [0.00980392],\n",
       "       [0.03921569],\n",
       "       [0.02941176],\n",
       "       [0.04901961],\n",
       "       [0.        ],\n",
       "       [0.05882353],\n",
       "       [1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data-data.min())/(data.max() - data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T03:00:39.008364Z",
     "start_time": "2018-06-19T03:00:38.997384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3328055 ],\n",
       "       [-0.3328055 ],\n",
       "       [-0.36642224],\n",
       "       [-0.40003896],\n",
       "       [-0.2991888 ],\n",
       "       [-0.3328055 ],\n",
       "       [-0.26557207],\n",
       "       [-0.43365568],\n",
       "       [-0.23195536],\n",
       "       [ 2.9952497 ]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-19T03:00:40.658618Z",
     "start_time": "2018-06-19T03:00:40.648478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3328055 ],\n",
       "       [-0.3328055 ],\n",
       "       [-0.36642224],\n",
       "       [-0.40003896],\n",
       "       [-0.2991888 ],\n",
       "       [-0.3328055 ],\n",
       "       [-0.26557207],\n",
       "       [-0.43365568],\n",
       "       [-0.23195536],\n",
       "       [ 2.9952497 ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data-data.mean())/data.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>sklearn.preprocessing.Imputer()</code><br />\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is reasonable to say that features with low variance are worse than those with high variance. So, one can consider cutting features with variance below a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T10:05:49.696535Z",
     "start_time": "2018-06-17T10:05:49.680916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# generate a ndarray of size (100, 20)\n",
    "x_data_generated, y_data_generated = make_classification()\n",
    "\n",
    "x_data_generated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T10:08:02.580260Z",
     "start_time": "2018-06-17T10:08:02.564638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_data_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T10:06:41.543224Z",
     "start_time": "2018-06-17T10:06:41.527601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 19)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VarianceThreshold(0.7).fit_transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T10:06:54.321463Z",
     "start_time": "2018-06-17T10:06:54.305843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VarianceThreshold(0.8).fit_transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T10:07:01.406476Z",
     "start_time": "2018-06-17T10:07:01.390854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VarianceThreshold(0.9).fit_transform(x_data_generated).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-17T10:47:25.491044Z",
     "start_time": "2018-06-17T10:47:25.475423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n",
      "(150, 4)\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[0.82530129 0.43214658 1.75852918 0.76061262]\n",
      "**************************************************\n",
      "(150, 3)\n",
      "[[5.1 1.4 0.2]\n",
      " [4.9 1.4 0.2]\n",
      " [4.7 1.3 0.2]\n",
      " [4.6 1.5 0.2]\n",
      " [5.  1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# https://chrisalbon.com/machine_learning/feature_selection/variance_thresholding_for_feature_selection/\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Load iris data\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(type(iris))\n",
    "\n",
    "# Create features and target\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(X.shape)\n",
    "print(X[0:5])\n",
    "print(X.std(axis=0))\n",
    "print('*'*50)\n",
    "\n",
    "# Create VarianceThreshold object with a variance with a threshold of 0.5\n",
    "thresholder = VarianceThreshold(threshold=.5)\n",
    "\n",
    "# Conduct variance thresholding\n",
    "X_high_variance = thresholder.fit_transform(X)\n",
    "\n",
    "print(X_high_variance.shape)\n",
    "\n",
    "# View first five rows with features with variances above threshold\n",
    "print(X_high_variance[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Very Important: </b>\n",
    "1. If variables represent different physical quantities their scaling can be different. By changing units (e.g. from measuring distance in kilometers to measuring distance in nanometers) you can change the scaling of a variable arbitrarily.\n",
    "\n",
    "2. If the variance is zero, it means that the feature is constant and will not improve the performance of the model. In that case, it should be removed. Or if only a handful of observations differ from a constant value, the variance will also be very low.\n",
    "\n",
    "3. If there is high correlation between 2 features then you would discard one of them. The features that are removed because of low variance have very low variance, that would be near to zero. You should always perform all the tests with existing data before discarding any features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search\n",
    "(covered later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation & p-value\n",
    "(covered later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Topics Left</b>\n",
    "\n",
    "1. CDF\n",
    "2. shapiro\n",
    "3. feature selection - grid search\n",
    "4. qq plot\n",
    "5. pearsonr\n",
    "6. Missing values using classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
